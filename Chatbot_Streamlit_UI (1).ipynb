{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuvcfb7KtIyJ"
      },
      "outputs": [],
      "source": [
        "pip install openai==0.28\n",
        "pip install streamlit streamlit-webrtc librosa scikit-learn xgboost speechrecognition pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace with ngrok token\n",
        "ngrok.set_auth_token(\"key\")\n",
        "\n",
        "! pip install streamlit -q\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "! streamlit run app.py & npx localtunnel --port 8501\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "App.py"
      ],
      "metadata": {
        "id": "b-Sxzlv85vZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import librosa\n",
        "import numpy as np\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "import openai\n",
        "\n",
        "# Load models\n",
        "xgb_model = joblib.load('/content/drive/MyDrive/xgb_emotion_model.pkl')\n",
        "cnn_model = tf.keras.models.load_model('/content/drive/MyDrive/cnn_emotion_model.h5')\n",
        "label_encoder = joblib.load('/content/drive/MyDrive/label_encoder.pkl')\n",
        "\n",
        "# OpenAI API key\n",
        "openai.api_key =\"api_key\"\n",
        "\n",
        "# Session state init\n",
        "if \"conversation_history\" not in st.session_state:\n",
        "    st.session_state.conversation_history = []\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"current_emotion\" not in st.session_state:\n",
        "    st.session_state.current_emotion = None\n",
        "if \"audio_file_name\" not in st.session_state:\n",
        "    st.session_state.audio_file_name = None\n",
        "if \"chat_ready\" not in st.session_state:\n",
        "    st.session_state.chat_ready = False\n",
        "\n",
        "# Feature extraction\n",
        "def extract_features_flat(file_path, max_pad_len=174):\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "    pad_width = max_pad_len - mfccs.shape[1]\n",
        "    mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant') if pad_width > 0 else mfccs[:, :max_pad_len]\n",
        "    return mfccs.flatten().reshape(1, -1)\n",
        "\n",
        "def extract_features_2d(file_path, max_pad_len=174):\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "    pad_width = max_pad_len - mfccs.shape[1]\n",
        "    mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant') if pad_width > 0 else mfccs[:, :max_pad_len]\n",
        "    return mfccs[..., np.newaxis].reshape(1, 40, max_pad_len, 1)\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title(\"üóÇÔ∏è Conversations\")\n",
        "for i, convo in enumerate(st.session_state.conversation_history):\n",
        "    label = f\"{i + 1}. {convo['audio_file']} ({convo['emotion']})\"\n",
        "    if st.sidebar.button(label, key=f\"sidebar_{i}\"):\n",
        "        st.session_state.messages = convo[\"chat\"]\n",
        "        st.session_state.current_emotion = convo[\"emotion\"]\n",
        "        st.session_state.audio_file_name = convo[\"audio_file\"]\n",
        "        st.session_state.chat_ready = True\n",
        "\n",
        "if st.sidebar.button(\"‚ûï New Chat\"):\n",
        "    st.session_state.clear()\n",
        "    st.session_state.conversation_history = []\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.current_emotion = None\n",
        "    st.session_state.audio_file_name = None\n",
        "    st.session_state.chat_ready = False\n",
        "    st.rerun()\n",
        "\n",
        "# Title\n",
        "st.title(\"üéôÔ∏è Emotion-Aware ChatGPT\")\n",
        "\n",
        "# Step 1: Upload and detect emotion\n",
        "if not st.session_state.chat_ready:\n",
        "    st.subheader(\"Step 1: Upload Audio to Detect Emotion\")\n",
        "    model_choice = st.radio(\"Select Model\", [\"XGBoost\", \"CNN\"])\n",
        "    uploaded_file = st.file_uploader(\"Upload a .wav file\", type=[\"wav\"])\n",
        "\n",
        "    if uploaded_file:\n",
        "        with open(\"temp.wav\", \"wb\") as f:\n",
        "            f.write(uploaded_file.read())\n",
        "\n",
        "        features = extract_features_flat(\"temp.wav\") if model_choice == \"XGBoost\" else extract_features_2d(\"temp.wav\")\n",
        "        pred = xgb_model.predict(features)[0] if model_choice == \"XGBoost\" else np.argmax(cnn_model.predict(features), axis=1)[0]\n",
        "        emotion = label_encoder.inverse_transform([pred])[0]\n",
        "\n",
        "        st.session_state.current_emotion = emotion\n",
        "        st.session_state.audio_file_name = uploaded_file.name\n",
        "        st.session_state.messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"You are a helpful assistant. The user is feeling {emotion}. Be supportive and empathetic.\"\n",
        "            }\n",
        "        ]\n",
        "        st.session_state.chat_ready = True\n",
        "        st.success(f\"Detected emotion: {emotion}\")\n",
        "\n",
        "# Step 2: Chat UI\n",
        "if st.session_state.chat_ready:\n",
        "    st.subheader(\"üí¨ Chat with Assistant\")\n",
        "\n",
        "    # Display chat history\n",
        "    for msg in st.session_state.messages:\n",
        "        if msg[\"role\"] == \"system\":\n",
        "            continue\n",
        "        sender = \"üßë You\" if msg[\"role\"] == \"user\" else \"ü§ñ Assistant\"\n",
        "        st.markdown(f\"**{sender}:** {msg['content']}\")\n",
        "\n",
        "    # Chat input (always visible)\n",
        "    user_input = st.chat_input(\"Say something...\")\n",
        "    if user_input:\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=st.session_state.messages\n",
        "        )\n",
        "        assistant_reply = response.choices[0].message[\"content\"]\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "\n",
        "        # Save to conversation history (latest state)\n",
        "        st.session_state.conversation_history.append({\n",
        "            \"emotion\": st.session_state.current_emotion,\n",
        "            \"audio_file\": st.session_state.audio_file_name,\n",
        "            \"chat\": st.session_state.messages.copy()\n",
        "        })\n",
        "\n",
        "        st.rerun()\n"
      ],
      "metadata": {
        "id": "m01Rlu3H5wqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}